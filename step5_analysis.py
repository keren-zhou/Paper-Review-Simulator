# step5_analysis.py
# -*- coding: utf-8 -*-

# ==============================================================================
# 1. IMPORTS
# ==============================================================================
import openreview
import json
import os
import dill
import pandas as pd
from typing import List, Dict, Any
import argparse
import configparser
import re  # <--- æ–°å¢: å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—

# For semantic filtering functionality
from sklearn.metrics.pairwise import cosine_similarity
from langchain_huggingface import HuggingFaceEmbeddings

# ==============================================================================
# 2. æ ¸å¿ƒåŠŸèƒ½ä»£ç  (æ¥è‡ªæ‚¨çš„é¡¹ç›®)
# ==============================================================================

# --- Utility Functions ---
def get_client(email: str, password: str):
    """æ ¹æ®ä¼ å…¥çš„å‡­è¯åˆå§‹åŒ– OpenReview å®¢æˆ·ç«¯ã€‚"""
    try:
        client_v1 = openreview.Client(baseurl='https://api.openreview.net', username=email, password=password)
        client_v2 = openreview.api.OpenReviewClient(baseurl='https://api2.openreview.net', username=email, password=password)
        print("âœ… OpenReview å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸã€‚")
        return client_v1, client_v2
    except Exception as e:
        print(f"âŒ é”™è¯¯: åˆå§‹åŒ– OpenReview å®¢æˆ·ç«¯å¤±è´¥: {e}")
        print("   è¯·æ£€æŸ¥æ‚¨åœ¨ config.ini æ–‡ä»¶ä¸­é…ç½®çš„ EMAIL å’Œ PASSWORDã€‚")
        raise SystemExit

def papers_to_list(papers_dict: Dict) -> List:
    all_papers = []
    for group in papers_dict.values():
        for venue_papers in group.values():
            all_papers.extend(venue_papers)
    return all_papers

def to_csv_pandas(papers_list: List[Dict], fpath: str):
    if not papers_list:
        print("âš ï¸ è­¦å‘Š: æ²¡æœ‰è®ºæ–‡å¯ä»¥ä¿å­˜åˆ° CSVã€‚")
        return
    try:
        df = pd.DataFrame(papers_list)
        df.to_csv(fpath, index=False, encoding='utf-8-sig')
        print(f"âœ… æˆåŠŸä¿å­˜ {len(df)} ç¯‡è®ºæ–‡åˆ° '{fpath}'")
    except Exception as e:
        print(f"âŒ é”™è¯¯: ä¿å­˜ CSV æ–‡ä»¶å¤±è´¥: {e}")

def save_papers(papers: Any, fpath: str):
    with open(fpath, 'wb') as fp:
        dill.dump(papers, fp)
    print(f"   -> åŸå§‹è®ºæ–‡æ•°æ®å·²ç¼“å­˜è‡³: {fpath}")

def load_papers(fpath: str) -> Any:
    with open(fpath, 'rb') as fp:
        papers = dill.load(fp)
    print(f"   -> ä»ç¼“å­˜åŠ è½½åŸå§‹è®ºæ–‡æ•°æ®: {fpath}")
    return papers

# --- Venue Functions ---
def get_venues(clients, confs, years):
    """
    ä»OpenReviewè·å–æ‰€æœ‰ä¼šè®®IDï¼Œå¹¶æ ¹æ®æä¾›çš„ä¼šè®®ç¼©å†™åˆ—è¡¨è¿›è¡Œç²¾ç¡®åŒ¹é…ã€‚
    """
    client_v1, client_v2 = clients
    all_venues = set()
    try:
        venues_v1 = client_v1.get_group(id='venues').members
        all_venues.update(venues_v1)
    except Exception: pass
    try:
        venues_v2 = client_v2.get_group(id='venues').members
        all_venues.update(venues_v2)
    except Exception: pass

    # ==========================================================================
    # --- [ä¼˜åŒ–æ ¸å¿ƒ] START ---
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å’Œå•è¯è¾¹ç•Œ(\b)æ¥ç¡®ä¿ç²¾ç¡®åŒ¹é…ï¼Œé¿å…å­å­—ç¬¦ä¸²è¯¯åŒ¹é…ã€‚
    # ==========================================================================
    reqd_venues = []
    for venue in all_venues:
        for conf in confs:
            # åˆ›å»ºä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼ï¼Œç¡®ä¿confæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å•è¯æˆ–è¢«éå­—æ¯æ•°å­—å­—ç¬¦åŒ…å›´
            # ä¾‹å¦‚ï¼ŒæŸ¥æ‰¾ "RE" æ—¶ï¼Œå®ƒä¼šåŒ¹é… "RE/2024"ï¼Œä½†ä¸ä¼šåŒ¹é… "NeurIPS" æˆ– "Conference"
            # re.escape() ä¼šå¤„ç†ç‰¹æ®Šå­—ç¬¦ï¼Œä¾‹å¦‚ CODES+ISSS ä¸­çš„ '+'
            pattern = r'\b' + re.escape(conf) + r'\b'
            
            # ä½¿ç”¨ re.search è¿›è¡Œä¸åŒºåˆ†å¤§å°å†™çš„åŒ¹é…
            if re.search(pattern, venue, re.IGNORECASE):
                # å¦‚æœåŒ¹é…æˆåŠŸï¼Œå†æ£€æŸ¥å¹´ä»½
                for year in years:
                    if year in venue:
                        reqd_venues.append(venue)
                        break # æ‰¾åˆ°å¹´ä»½åï¼Œæ— éœ€å†æ£€æŸ¥æ­¤ä¼šè®®çš„å…¶ä»–å¹´ä»½
                break # æ‰¾åˆ°ä¼šè®®åï¼Œæ— éœ€å†æ£€æŸ¥æ­¤venueæ˜¯å¦åŒ¹é…å…¶ä»–confç¼©å†™
    # ==========================================================================
    # --- [ä¼˜åŒ–æ ¸å¿ƒ] END ---
    # ==========================================================================
            
    return list(set(reqd_venues))


def group_venues(venues, bins):
    bins_dict = {bin_name: [] for bin_name in bins}
    for venue in venues:
        for bin_name in bins:
            # è¿™é‡Œçš„åŒ¹é…é€»è¾‘ä¹ŸåŒæ ·å¯ä»¥ä¼˜åŒ–ï¼Œä»¥æé«˜åˆ†ç»„çš„å‡†ç¡®æ€§
            pattern = r'\b' + re.escape(bin_name) + r'\b'
            if re.search(pattern, venue, re.IGNORECASE):
                bins_dict[bin_name].append(venue)
                break
    return bins_dict

# --- Paper Fetching ---
def get_papers(clients, grouped_venues):
    _, client_v2 = clients
    papers = {}
    for group, venues in grouped_venues.items():
        if not venues: # å¦‚æœåˆ†ç»„åæŸä¸ªä¼šè®®æ²¡æœ‰æ‰¾åˆ°ä»»ä½•venueï¼Œåˆ™è·³è¿‡
            continue
        papers[group] = {}
        for venue in venues:
            print(f"   -> æ­£åœ¨æŸ¥è¯¢ä¼šè®®: {venue}...")
            try:
                submissions = client_v2.get_all_notes(content={'venueid': venue}, details='directReplies')
                papers[group][venue] = submissions
                print(f"      æ‰¾åˆ° {len(submissions)} ç¯‡å·²æ¥æ”¶çš„è®ºæ–‡ã€‚")
            except Exception as e:
                print(f"      âš ï¸ è­¦å‘Š: æ— æ³•è·å– {venue} çš„è®ºæ–‡ã€‚é”™è¯¯: {e}")
                papers[group][venue] = []
    return papers

# --- Filtering Logic ---
def check_keywords_with_text(keywords, text):
    if not text or not keywords: return None, False
    text_lower = str(text).lower()
    for keyword in keywords:
        if str(keyword).lower() in text_lower:
            return keyword, True
    return None, False

def title_filter(paper, keywords):
    title = paper.content.get('title', {}).get('value', '')
    return check_keywords_with_text(keywords, title)

def abstract_filter(paper, keywords):
    abstract = paper.content.get('abstract', {}).get('value', '')
    return check_keywords_with_text(keywords, abstract)

def satisfies_any_filters(paper, keywords, filters):
    for filter_func in filters:
        matched_keyword, matched = filter_func(paper, keywords)
        if matched:
            return matched_keyword, filter_func.__name__, True
    return None, None, False

# --- Data Extractor ---
class Extractor:
    def __init__(self, fields, subfields, details_subfields):
        self.fields = fields
        self.subfields = subfields
        self.details_subfields = details_subfields
  
    def __call__(self, paper):
        trimmed = {}
        for field in self.fields:
            trimmed[field] = getattr(paper, field, None)
        
        content = getattr(paper, 'content', {})
        for field in self.subfields.get('content', []):
            value_dict = content.get(field, {})
            trimmed[field] = value_dict.get('value') if isinstance(value_dict, dict) else value_dict
            
        if self.details_subfields and hasattr(paper, 'details'):
            for field in self.details_subfields: trimmed[f"review_{field}"] = []
            
            for reply in paper.details.get('directReplies', []):
                is_review = any('Official_Review' in inv for inv in reply.get('invitations', []))
                if is_review:
                    for field in self.details_subfields:
                        value_dict = reply.get('content', {}).get(field, {})
                        value = value_dict.get('value') if isinstance(value_dict, dict) else value_dict
                        if value: trimmed[f"review_{field}"].append(str(value))

            for field in self.details_subfields:
                trimmed[f"review_{field}"] = " ||| ".join(trimmed[f"review_{field}"])
        
        return trimmed

# --- Scraper Class (ä¿®æ”¹å) ---
class Scraper:
    def __init__(self, conferences, years, keywords, extractor, email, password, fns=[]):
        self.confs = conferences
        self.years = years
        self.keywords = keywords
        self.extractor = extractor
        self.fns = fns
        self.filters = []
        self.clients = get_client(email, password)
        self.papers = None

    def add_filter(self, filter_func):
        self.filters.append(filter_func)
  
    def __call__(self):
        print("\n[æ­¥éª¤ 2] æ­£åœ¨æŸ¥æ‰¾åŒ¹é…çš„ OpenReview ä¼šè®®...")
        venues = get_venues(self.clients, self.confs, self.years)
        if not venues:
            print("âŒ æœªåœ¨ OpenReview ä¸Šæ‰¾åˆ°ä¸ç»™å®šä¼šè®®å’Œå¹´ä»½åŒ¹é…çš„æ¡ç›®ã€‚")
            self.papers = {}
            return
        print(f"   æ‰¾åˆ° {len(venues)} ä¸ªç›¸å…³çš„ä¼šè®® IDã€‚")

        print("\n[æ­¥éª¤ 3] æ­£åœ¨ä»ä¼šè®®ä¸­è·å–è®ºæ–‡æ•°æ® (è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
        grouped = group_venues(venues, self.confs)
        papers_data = get_papers(self.clients, grouped)
        
        print("\n[æ­¥éª¤ 4] æ­£åœ¨åº”ç”¨åˆå§‹å…³é”®è¯è¿‡æ»¤å™¨å¹¶æå–æ•°æ®...")
        extracted_papers = {}
        count = 0
        for group, venues_data in papers_data.items():
            extracted_papers[group] = {}
            for venue, paper_list in venues_data.items():
                extracted_papers[group][venue] = []
                for paper in paper_list:
                    match_kw, match_type, satisfies = satisfies_any_filters(paper, self.keywords, self.filters)
                    if satisfies:
                        for fn in self.fns: paper = fn(paper)
                        
                        extracted = self.extractor(paper)
                        extracted['conference'] = group
                        extracted['match_keyword'] = match_kw
                        extracted['match_type'] = match_type
                        
                        extracted_papers[group][venue].append(extracted)
                        count += 1
        
        self.papers = extracted_papers
        print(f"   åˆå§‹è¿‡æ»¤å®Œæˆã€‚æ‰¾åˆ° {count} ç¯‡å¯èƒ½ç›¸å…³çš„è®ºæ–‡ã€‚")

# ==============================================================================
# 3. æµæ°´çº¿é›†æˆåŠŸèƒ½ (æ–°å¢ä¸ä¿®æ”¹)
# ==============================================================================

def generate_search_topic_and_keywords_from_json(analysis_json_path: str) -> (str, List[str]):
    """ä» step2 çš„åˆ†ææ–‡ä»¶ä¸­åŠ¨æ€ç”Ÿæˆé«˜è´¨é‡çš„æœç´¢ä¸»é¢˜å’Œå…³é”®è¯åˆ—è¡¨ã€‚"""
    print("\n[æ­¥éª¤ 0] æ­£åœ¨ä»åˆ†ææ–‡ä»¶ä¸­åŠ¨æ€ç”Ÿæˆæœç´¢ä¸»é¢˜å’Œå…³é”®è¯...")
    try:
        with open(analysis_json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        summary = data.get("paper_summary", {})
        problem = summary.get("problem_statement", "a novel problem")
        innovations = [item.get("innovation_name", "unnamed innovation") for item in summary.get("key_innovations", [])]
        keywords = summary.get("keywords", [])
        
        search_topic = f"A research paper addressing the problem of '{problem}'. Key innovations include {', '.join(innovations)}. Core concepts are {', '.join(keywords)}."
        
        print(f"   -> å·²ç”Ÿæˆæœç´¢ä¸»é¢˜: \"{search_topic}\"")
        print(f"   -> å°†ä½¿ç”¨å…³é”®è¯è¿›è¡Œé¢„è¿‡æ»¤: {keywords}")
        return search_topic, keywords
    except Exception as e:
        print(f"   âŒ é”™è¯¯: æ— æ³•ä» JSON æ–‡ä»¶ç”Ÿæˆæœç´¢ä¸»é¢˜: {e}")
        return "", []

def load_conferences_for_tier(filepath: str, target_tier: str) -> List[str]:
    """ä» JSON çŸ¥è¯†åº“ä¸­ä¸ºç‰¹å®šç­‰çº§åŠ è½½ä¼šè®®ç¼©å†™ã€‚"""
    print(f"\n[æ­¥éª¤ 1] æ­£åœ¨ä» '{filepath}' åŠ è½½ '{target_tier}' ç­‰çº§çš„ä¼šè®®...")
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            all_confs = json.load(f)
        
        target_confs = [
            conf['venue_abbr'] for conf in all_confs if conf.get('tier') == target_tier
        ]
        
        if not target_confs:
            print(f"   âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ° '{target_tier}' ç­‰çº§çš„ä¼šè®®ã€‚")
        else:
            print(f"   -> æ‰¾åˆ° {len(target_confs)} ä¸ªä¼šè®®: {', '.join(target_confs)}")
        return target_confs
    except FileNotFoundError:
        print(f"   âŒ é”™è¯¯: çŸ¥è¯†åº“æ–‡ä»¶æœªåœ¨ '{filepath}' æ‰¾åˆ°ã€‚")
        return []
    except json.JSONDecodeError:
        print(f"   âŒ é”™è¯¯: æ–‡ä»¶ '{filepath}' ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ–‡ä»¶ã€‚")
        return []

def filter_by_semantic_similarity(papers_list: List[Dict], topic: str, threshold: float, model_name: str) -> List[Dict]:
    """æ ¹æ®ä¸ç»™å®šä¸»é¢˜çš„è¯­ä¹‰ç›¸ä¼¼åº¦è¿‡æ»¤è®ºæ–‡åˆ—è¡¨ã€‚"""
    if not papers_list:
        return []

    print("\n[æ­¥éª¤ 5] æ­£åœ¨æ‰§è¡Œç²¾ç¡®çš„è¯­ä¹‰è¿‡æ»¤...")
    print(f"   -> æ ¸å¿ƒä¸»é¢˜: '{topic}'")
    print(f"   -> ç›¸ä¼¼åº¦é˜ˆå€¼: {threshold}")

    try:
        print(f"   -> æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹ ({model_name})...")
        model = HuggingFaceEmbeddings(model_name=model_name)
    except Exception as e:
        print(f"   âŒ é”™è¯¯: æ— æ³•åŠ è½½åµŒå…¥æ¨¡å‹ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚é”™è¯¯: {e}")
        return []

    print(f"   -> æ­£åœ¨ä¸º {len(papers_list)} ç¯‡è®ºæ–‡åˆ›å»ºæ–‡æœ¬åµŒå…¥...")
    paper_texts = [f"{p.get('title', '')}\n{p.get('abstract', '')}" for p in papers_list]
    
    query_embedding = model.embed_query(topic)
    paper_embeddings = model.embed_documents(paper_texts)
    
    print("   -> æ­£åœ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦...")
    similarities = cosine_similarity([query_embedding], paper_embeddings)[0]
    
    highly_relevant_papers = []
    for i, paper in enumerate(papers_list):
        score = similarities[i]
        if score >= threshold:
            paper['similarity_score'] = round(score, 4)
            highly_relevant_papers.append(paper)
    
    highly_relevant_papers.sort(key=lambda x: x['similarity_score'], reverse=True)
    
    print(f"   âœ… è¯­ä¹‰è¿‡æ»¤å®Œæˆã€‚æ‰¾åˆ° {len(highly_relevant_papers)} ç¯‡é«˜åº¦ç›¸å…³çš„è®ºæ–‡ã€‚")
    return highly_relevant_papers

def filter_papers_with_reviews(papers_list: List[Dict], review_fields: List[str]) -> List[Dict]:
    """æœ€ç»ˆè¿‡æ»¤æ­¥éª¤ï¼Œç¡®ä¿åªä¿ç•™å…·æœ‰å®é™…ã€éç©ºå®¡ç¨¿å†…å®¹çš„è®ºæ–‡ã€‚"""
    print("\n[æ­¥éª¤ 6] æœ€ç»ˆæ£€æŸ¥: è¿‡æ»¤åŒ…å«å®¡ç¨¿å†…å®¹çš„è®ºæ–‡...")
    papers_with_actual_reviews = []
    for paper in papers_list:
        has_any_review_content = False
        for field in review_fields:
            review_key = f"review_{field}"
            if paper.get(review_key, "").strip():
                has_any_review_content = True
                break
        
        if has_any_review_content:
            papers_with_actual_reviews.append(paper)

    print(f"   -> æ‰¾åˆ° {len(papers_with_actual_reviews)} ç¯‡å…·æœ‰å¯è®¿é—®å®¡ç¨¿æ„è§çš„è®ºæ–‡ã€‚")
    return papers_with_actual_reviews

# ==============================================================================
# 4. ä¸»æ‰§è¡Œæµç¨‹ (å®Œå…¨é‡æ„ä»¥é€‚åº”æµæ°´çº¿)
# ==============================================================================

def run_openreview_scraper(
    analysis_json_path: str,
    output_csv_path: str,
    target_tier: str,
    email: str,
    password: str,
    similarity_threshold: float,
    max_papers: int,
    embedding_model_name: str,
    search_years: List[str]
):
    """
    åè°ƒæ•´ä¸ª OpenReview æŠ“å–å’Œè¿‡æ»¤æµç¨‹çš„ä¸»å‡½æ•°ã€‚
    """
    output_dir = os.path.dirname(output_csv_path)
    papers_cache_path = os.path.join(output_dir, 'raw_papers_cache.pkl')

    search_topic, pre_filter_keywords = generate_search_topic_and_keywords_from_json(analysis_json_path)
    if not search_topic or not pre_filter_keywords:
        print("âŒ æ— æ³•ä»åˆ†ææ–‡ä»¶ç”Ÿæˆæœç´¢å‚æ•°ï¼Œæµç¨‹ç»ˆæ­¢ã€‚")
        return

    target_conferences = load_conferences_for_tier('venue_knowledge_base_ccf_auto.json', target_tier)
    
    if not target_conferences:
        print("âŒ æœªæ‰¾åˆ°ç›®æ ‡ä¼šè®®ï¼Œæµç¨‹ç»ˆæ­¢ã€‚")
        return

    review_fields_to_extract = ['rating', 'confidence', 'summary', 'strengths', 'weaknesses']
    extractor = Extractor(
        fields=['forum'], 
        subfields={'content': ['title', 'keywords', 'abstract', 'pdf']},
        details_subfields=review_fields_to_extract
    )

    def modify_paper_links(paper):
        """è¾…åŠ©å‡½æ•°ï¼Œåˆ›å»ºå®Œæ•´çš„ URLã€‚"""
        paper.forum = f"https://openreview.net/forum?id={paper.forum}"
        pdf_val = paper.content.get('pdf', {}).get('value')
        if pdf_val: paper.content['pdf']['value'] = f"https://openreview.net{pdf_val}"
        return paper

    if os.path.exists(papers_cache_path):
         print("\n[ç¼“å­˜] å‘ç°å·²å­˜åœ¨çš„åŸå§‹è®ºæ–‡ç¼“å­˜ã€‚ä»æ–‡ä»¶åŠ è½½ã€‚")
         all_papers_raw = load_papers(papers_cache_path)
    else:
        scraper = Scraper(
            conferences=target_conferences, 
            years=search_years, 
            keywords=pre_filter_keywords, 
            extractor=extractor, 
            email=email,
            password=password,
            fns=[modify_paper_links]
        )
        scraper.add_filter(title_filter)
        scraper.add_filter(abstract_filter)
        
        scraper()
        
        all_papers_raw = scraper.papers
        if all_papers_raw:
            save_papers(all_papers_raw, papers_cache_path)

    initial_paper_list = papers_to_list(all_papers_raw)
    
    semantically_relevant_papers = filter_by_semantic_similarity(
        papers_list=initial_paper_list,
        topic=search_topic,
        threshold=similarity_threshold,
        model_name=embedding_model_name
    )
    
    final_papers_with_reviews = filter_papers_with_reviews(
        papers_list=semantically_relevant_papers,
        review_fields=review_fields_to_extract
    )
    
    print(f"\n[æ­¥éª¤ 7] æ­£åœ¨ä¸ºæœ€ç»ˆæŠ¥å‘Šé€‰æ‹©å‰ {max_papers} ç¯‡æœ€ç›¸å…³çš„è®ºæ–‡...")
    top_papers = final_papers_with_reviews[:max_papers]
    print(f"   -> æ‰€æœ‰è¿‡æ»¤å™¨åæ€»è®ºæ–‡æ•°: {len(final_papers_with_reviews)}")
    print(f"   -> ä¸ºæœ€ç»ˆæŠ¥å‘Šé€‰æ‹©çš„è®ºæ–‡æ•°: {len(top_papers)}")

    print("\n[æ­¥éª¤ 8] æ­£åœ¨å°†æœ€ç»ˆç»“æœä¿å­˜åˆ° CSV...")
    to_csv_pandas(top_papers, output_csv_path)
    
    print("\nğŸ‰ Step 5 æµç¨‹æˆåŠŸå®Œæˆï¼")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Step 5: ä» OpenReview æŠ“å–ç›¸å…³çš„è®ºæ–‡å’Œå®¡ç¨¿æ„è§ã€‚")
    parser.add_argument("--analysis_json_path", type=str, required=True, help="æŒ‡å‘ comprehensive_analysis.json æ–‡ä»¶çš„è·¯å¾„ã€‚")
    parser.add_argument("--output_csv_path", type=str, required=True, help="ä¿å­˜è¾“å‡ºçš„ final_relevant_papers.csv æ–‡ä»¶çš„è·¯å¾„ã€‚")
    parser.add_argument("--target_tier", type=str, required=True, help="ç›®æ ‡ä¼šè®®ç­‰çº§ (ä¾‹å¦‚: CCF-A, CCF-B, CCF-C)ã€‚")
    parser.add_argument("--config", type=str, default='config.ini', help="é…ç½®æ–‡ä»¶è·¯å¾„ã€‚")
    args = parser.parse_args()

    config = configparser.ConfigParser()
    config.read(args.config)

    EMAIL = os.getenv('OPENREVIEW_EMAIL')
    PASSWORD = os.getenv('OPENREVIEW_PASSWORD')
    SIMILARITY_THRESHOLD = float(config['SETTINGS']['SIMILARITY_THRESHOLD'])
    MAX_PAPERS_IN_REPORT = int(config['SETTINGS']['MAX_PAPERS_OPENREVIEW'])
    EMBEDDING_MODEL_NAME = config['PATHS']['EMBEDDING_MODEL_PATH']
    
    SEARCH_YEARS = [year.strip() for year in config['SETTINGS']['OPENREVIEW_SEARCH_YEARS'].split(',')]

    if not EMAIL or not PASSWORD:
        print("âŒ å…³é”®é”™è¯¯: è¯·åœ¨è¿è¡Œå‰ï¼Œåœ¨ config.ini æ–‡ä»¶ä¸­è®¾ç½®æ‚¨çš„ OpenReview EMAIL å’Œ PASSWORDã€‚")
    else:
        run_openreview_scraper(
            analysis_json_path=args.analysis_json_path,
            output_csv_path=args.output_csv_path,
            target_tier=args.target_tier,
            email=EMAIL,
            password=PASSWORD,
            similarity_threshold=SIMILARITY_THRESHOLD,
            max_papers=MAX_PAPERS_IN_REPORT,
            embedding_model_name=EMBEDDING_MODEL_NAME,
            search_years=SEARCH_YEARS
        )