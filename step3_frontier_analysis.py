# step3_frontier_analysis.py (migrated from step4_frontier_analysis.py)
# -*- coding: utf-8 -*-

# ==============================================================================
# 1. IMPORTS
# ==============================================================================
import openai
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
import json
import time
import datetime
import calendar
import arxiv
import threading
import queue
import asyncio
from pathlib import Path
from typing import Dict, List, Any
import argparse
import configparser

from sklearn.metrics.pairwise import cosine_similarity
from langchain_huggingface import HuggingFaceEmbeddings

# --- å…¨å±€å˜é‡ ---
qwen_client = None
async_qwen_client = None
embedding_model = None

# ==============================================================================
# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° (å·²ä¿®å¤) ---
# ==============================================================================

def load_analysis_data(analysis_json_path: Path) -> Dict[str, Any] | None:
    if not analysis_json_path.exists():
        print(f"é”™è¯¯ï¼šåˆ†ææ–‡ä»¶ä¸å­˜åœ¨äº '{analysis_json_path}'")
        return None
    try:
        with open(analysis_json_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"é”™è¯¯: åŠ è½½æˆ–è§£æJSONæ–‡ä»¶å¤±è´¥: {e}")
        return None

def generate_search_query_with_qwen(analysis_data: Dict[str, Any]) -> str | None:
    print("   [QueryGen] æ­£åœ¨å°è¯•ä½¿ç”¨åƒé—®å¤§æ¨¡å‹ç”Ÿæˆæ™ºèƒ½æœç´¢æŸ¥è¯¢...")
    summary = analysis_data.get("paper_summary", {})
    problem = summary.get("problem_statement", "")
    innovations = [f"- {item.get('innovation_name', '')}: {item.get('innovation_description', '')}" for item in summary.get("key_innovations", [])]
    keywords_text = ', '.join(summary.get("keywords", []))
    innovations_text = '\n'.join(innovations)
    prompt = f"""
You are an expert research assistant. Based on the following information from a research paper, generate a single, effective search query for the arXiv database to find the latest related research.
The query's goal is to be **BROAD** to maximize discovery. It should:
1. Identify 3-5 of the most important and distinct conceptual groups from the paper's information.
2. For each group, you can include alternative terms using `OR` (e.g., `"3D Gaussian Splatting" OR "3DGS"`).
3. Crucially, you MUST link these main conceptual groups together using the `OR` operator, not `AND`. This will find papers related to any of the core concepts.
4. Your output MUST be only the query string itself, without any explanation, labels, or quotation marks around the whole string.
--- Paper Information ---
Problem Statement: {problem}
Key Innovations:
{innovations_text}
Keywords: {keywords_text}
---
Example of a good BROAD query: `("3D Gaussian Splatting" OR "3DGS") OR ("compositional scene generation") OR ("physics-aware layout")`
Now, generate the broad query based on the paper information provided.
"""
    try:
        response = qwen_client.chat.completions.create(
            model="qwen-plus", messages=[{"role": "user", "content": prompt}], max_tokens=200, temperature=0.2,
        )
        query = response.choices[0].message.content.strip()
        if query.startswith('"') and query.endswith('"'): query = query[1:-1]
        print(f"   [QueryGen] âœ… æˆåŠŸç”Ÿæˆæ™ºèƒ½æŸ¥è¯¢ã€‚")
        return query
    except Exception as e:
        print(f"   [QueryGen] âŒ è°ƒç”¨åƒé—®ç”ŸæˆæŸ¥è¯¢å¤±è´¥: {e}")
        return None

# ==========================================================================
# --- [æ ¸å¿ƒä¿®å¤] ---
# ä¿®æ­£äº† construct_search_query å‡½æ•°ä¸­çš„ UnboundLocalError
# ==========================================================================
def construct_search_query(analysis_data: Dict[str, Any]) -> tuple[str, str]:
    """
    æ ¹æ®åˆ†ææ•°æ®æ„å»º arXiv æœç´¢æŸ¥è¯¢å’Œç”¨äºè¯­ä¹‰æ¯”è¾ƒçš„æ ¸å¿ƒæ–‡æœ¬ã€‚
    """
    summary = analysis_data.get("paper_summary", {})
    
    # --- ä¿®å¤å¼€å§‹ ---
    # æ— è®ºåç»­æ“ä½œå¦‚ä½•ï¼Œéƒ½å…ˆæ— æ¡ä»¶åœ°ä»æ‘˜è¦ä¸­æå–æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯
    problem = summary.get("problem_statement", "")
    innovations = [item.get("innovation_name", "") for item in summary.get("key_innovations", [])]
    keywords = summary.get("keywords", [])
    # --- ä¿®å¤ç»“æŸ ---

    arxiv_query = generate_search_query_with_qwen(analysis_data)
    
    if not arxiv_query:
        print("   [QueryGen] å¯åŠ¨å¤‡ç”¨æŸ¥è¯¢ç”Ÿæˆé€»è¾‘...")
        # ç°åœ¨ 'keywords' å˜é‡åœ¨æ­¤å¤„å¿…å®šå­˜åœ¨
        cleaned_keywords = sorted(list(set(kw.strip() for kw in keywords if kw)), key=len)
        core_keywords = cleaned_keywords[:5]
        arxiv_query = " OR ".join(f'"{kw}"' for kw in core_keywords)
        print(f"   [QueryGen] âœ… å·²ç”Ÿæˆå¤‡ç”¨æŸ¥è¯¢ã€‚")

    # ç°åœ¨ 'keywords' å˜é‡åœ¨è¿™ä¸€è¡Œä¹Ÿå¿…å®šå­˜åœ¨
    semantic_text = f"Problem: {problem}. Innovations: {'. '.join(innovations)}. Keywords: {', '.join(keywords)}"
    
    return arxiv_query, semantic_text

async def summarize_paper_with_qwen_async(title: str, abstract: str) -> Dict[str, Any] | None:
    prompt = f"""
You are a highly skilled AI assistant specializing in scientific literature. Your task is to summarize the following research paper based on its title and abstract.
Your summary MUST strictly follow this format:
"In the domain of [domain], to solve the problem of [problem], a method of [method A + method B] was proposed."
Do not add any other text, explanation, or introductory phrases. Your entire output should be a single JSON object containing one key "summary".
---
Title: {title}
Abstract: {abstract}
---
Output the result as a single JSON object like this: {{"summary": "In the domain of..."}}
"""
    try:
        response = await async_qwen_client.chat.completions.create(
            model="qwen-plus",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1024,
            temperature=0.0,
            response_format={"type": "json_object"}
        )
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        print(f"é”™è¯¯ï¼šå¼‚æ­¥æ€»ç»“å¤±è´¥ (è®ºæ–‡: '{title[:30]}...'): {e}")
        return None

# ==============================================================================
# --- 3. æµæ°´çº¿ä»»åŠ¡ (å¤šçº¿ç¨‹) ---
# ==============================================================================

def search_arxiv_task(query: str, start_date_str: str, raw_papers_queue: queue.Queue):
    print(f"--- [ç”Ÿäº§è€…çº¿ç¨‹å¯åŠ¨] ä½¿ç”¨æŸ¥è¯¢: '{query}' ---")
    start_date = datetime.datetime.strptime(start_date_str, "%Y-%m")
    end_date = datetime.datetime.now()
    client = arxiv.Client()
    seen_ids = set()
    for year in range(end_date.year, start_date.year - 1, -1):
        start_month = 1 if year > start_date.year else start_date.month
        end_month = end_date.month if year == end_date.year else 12
        for month in range(end_month, start_month - 1, -1):
            _, last_day = calendar.monthrange(year, month)
            start_of_month, end_of_month = f"{year}{month:02d}01", f"{year}{month:02d}{last_day}"
            print(f"--- [ç”Ÿäº§è€…] æ­£åœ¨æœç´¢: {year}-{month:02d} ---")
            query_with_date = f'({query}) AND submittedDate:[{start_of_month} TO {end_of_month}]'
            search = arxiv.Search(query=query_with_date, max_results=2000, sort_by=arxiv.SortCriterion.SubmittedDate)
            new_papers_this_month = []
            try:
                for result in client.results(search):
                    if result.entry_id not in seen_ids:
                        new_papers_this_month.append({
                            'title': result.title.replace('\n', ' ').strip(),
                            'summary': result.summary.replace('\n', ' ').strip(),
                            'id': result.entry_id, 'pdf_url': result.pdf_url
                        })
                        seen_ids.add(result.entry_id)
            except Exception as e:
                print(f"   [ç”Ÿäº§è€…] æœç´¢ {year}-{month:02d} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            if new_papers_this_month:
                print(f"   [ç”Ÿäº§è€…] -> å‘ç° {len(new_papers_this_month)} ç¯‡æ–°è®ºæ–‡ï¼Œæ”¾å…¥è¿‡æ»¤é˜Ÿåˆ—ã€‚")
                raw_papers_queue.put(new_papers_this_month)
            time.sleep(3)
    print("--- [ç”Ÿäº§è€…çº¿ç¨‹ç»“æŸ] æ‰€æœ‰æœˆä»½æœç´¢å®Œæ¯•ã€‚ ---")
    raw_papers_queue.put(None)

def filter_papers_task(raw_queue: queue.Queue, filtered_queue: queue.Queue, base_semantic_text: str, model, relevance_threshold: float):
    print(f"--- [è¿‡æ»¤å™¨çº¿ç¨‹å¯åŠ¨] ä½¿ç”¨é˜ˆå€¼ {relevance_threshold} ç­‰å¾…åŸå§‹æ•°æ®... ---")
    try:
        base_embedding = model.embed_query(base_semantic_text)
    except Exception as e:
        print(f"   [è¿‡æ»¤å™¨] ä¸¥é‡é”™è¯¯ï¼šæ— æ³•åˆ›å»ºåŸºå‡†åµŒå…¥ï¼é”™è¯¯: {e}. è¿‡æ»¤å™¨å°†æ— æ³•å·¥ä½œã€‚")
        raw_queue.get(); filtered_queue.put(None)
        return
    while True:
        paper_batch = raw_queue.get()
        if paper_batch is None:
            print("--- [è¿‡æ»¤å™¨çº¿ç¨‹ç»“æŸ] æ”¶åˆ°ç”Ÿäº§è€…ç»“æŸä¿¡å·ã€‚---")
            filtered_queue.put(None); break
        print(f"   [è¿‡æ»¤å™¨] <- æ”¶åˆ° {len(paper_batch)} ç¯‡è®ºæ–‡ï¼Œå¼€å§‹è¯­ä¹‰ç­›é€‰...")
        try:
            texts_to_embed = [f"Title: {p['title']}. Abstract: {p['summary']}" for p in paper_batch]
            paper_embeddings = model.embed_documents(texts_to_embed)
            similarities = cosine_similarity([base_embedding], paper_embeddings)[0]
        except Exception as e:
            print(f"   [è¿‡æ»¤å™¨] ä¸¥é‡é”™è¯¯ï¼šæ— æ³•ä¸ºæ‰¹æ¬¡åˆ›å»ºæ–‡æ¡£åµŒå…¥ï¼é”™è¯¯: {e}. è·³è¿‡æ­¤æ‰¹æ¬¡ã€‚")
            continue
        
        relevant_papers = []
        for i, paper in enumerate(paper_batch):
            if similarities[i] >= relevance_threshold:
                paper['relevance_score'] = float(similarities[i])
                relevant_papers.append(paper)

        if relevant_papers:
            print(f"   [è¿‡æ»¤å™¨] -> ç­›é€‰å‡º {len(relevant_papers)}/{len(paper_batch)} ç¯‡å¼ºç›¸å…³è®ºæ–‡ã€‚")
            filtered_queue.put(relevant_papers)

def summarize_papers_task(filtered_queue: queue.Queue, final_results_list: list):
    print("--- [æ€»ç»“å™¨çº¿ç¨‹å¯åŠ¨] ç­‰å¾…è¿‡æ»¤åçš„è®ºæ–‡... ---")
    async def process_batch_async(batch_to_process: List[Dict]):
        tasks = [summarize_paper_with_qwen_async(p['title'], p['summary']) for p in batch_to_process]
        summary_results = await asyncio.gather(*tasks, return_exceptions=True)
        for paper, result in zip(batch_to_process, summary_results):
            if isinstance(result, Exception) or not result or 'summary' not in result:
                print(f"       âŒ æ€»ç»“å¤±è´¥: '{paper['title'][:50]}...'")
            else:
                final_results_list.append({
                    "title": paper['title'], "arxiv_id": paper['id'], "pdf_url": paper['pdf_url'],
                    "relevance_score": paper.get('relevance_score', 0.0), "frontier_summary": result['summary']
                })
                print(f"       âœ… æ€»ç»“æˆåŠŸ: '{paper['title'][:50]}...'")

    while True:
        paper_batch = filtered_queue.get()
        if paper_batch is None:
            print("--- [æ€»ç»“å™¨çº¿ç¨‹ç»“æŸ] æ”¶åˆ°è¿‡æ»¤å™¨ç»“æŸä¿¡å·ã€‚---"); break
        print(f"   [æ€»ç»“å™¨] <- æ”¶åˆ° {len(paper_batch)} ç¯‡ç›¸å…³è®ºæ–‡ï¼Œå¼€å§‹å¹¶å‘æ€»ç»“...")
        start_time = time.time()
        asyncio.run(process_batch_async(paper_batch))
        end_time = time.time()
        print(f"   [æ€»ç»“å™¨] å®Œæˆ {len(paper_batch)} ç¯‡è®ºæ–‡çš„å¹¶å‘æ€»ç»“ï¼Œè€—æ—¶ {end_time - start_time:.2f} ç§’ã€‚")

# ==============================================================================
# --- 4. ä¸»æµç¨‹æ§åˆ¶å™¨ ---
# ==============================================================================
def run_frontier_analysis(
    analysis_json_path: str, output_json_path: str, max_papers: int,
    model_name: str, relevance_threshold: float, search_start_date: str
):
    global qwen_client, async_qwen_client, embedding_model
    try:
        qwen_client = openai.OpenAI(api_key=os.getenv("DASHSCOPE_API_KEY"), base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
        async_qwen_client = openai.AsyncOpenAI(api_key=os.getenv("DASHSCOPE_API_KEY"), base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
        print("--- åƒé—® Qwen API å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ ---")
    except Exception as e: print(f"åˆå§‹åŒ–åƒé—®å®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}"); return
    try:
        embedding_model = HuggingFaceEmbeddings(model_name=model_name)
        print(f"--- è¯­ä¹‰åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ ---\n")
    except Exception as e: print(f"é”™è¯¯: æ— æ³•åŠ è½½åµŒå…¥æ¨¡å‹ '{model_name}': {e}"); return

    analysis_file, output_file = Path(analysis_json_path), Path(output_json_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    print("--- æ­¥éª¤ 1/4: åŠ è½½åŸºç¡€åˆ†ææ–‡ä»¶ ---")
    analysis_data = load_analysis_data(analysis_file)
    if not analysis_data: return
    
    print("\n--- æ­¥éª¤ 2/4: æ„å»ºæœç´¢æŸ¥è¯¢ ---")
    arxiv_query, semantic_text = construct_search_query(analysis_data)
    if not arxiv_query: print("é”™è¯¯ï¼šæ— æ³•æ„å»ºæœ‰æ•ˆçš„æœç´¢æŸ¥è¯¢ã€‚"); return
    print(f"  â–¶ï¸ [æœ€ç»ˆä½¿ç”¨çš„ arXiv æŸ¥è¯¢]: {arxiv_query}\n")
    
    raw_papers_queue, filtered_papers_queue, final_results = queue.Queue(10), queue.Queue(10), []
    
    threads = [
        threading.Thread(target=search_arxiv_task, args=(arxiv_query, search_start_date, raw_papers_queue)),
        threading.Thread(target=filter_papers_task, args=(raw_papers_queue, filtered_papers_queue, semantic_text, embedding_model, relevance_threshold)),
        threading.Thread(target=summarize_papers_task, args=(filtered_papers_queue, final_results))
    ]
    print("--- æ­¥éª¤ 3/4: å¯åŠ¨ä¸‰é˜¶æ®µåˆ†ææµæ°´çº¿ ---\n")
    for t in threads: t.start()
    for t in threads: t.join()
    
    print("\n--- æ‰€æœ‰æµæ°´çº¿ä»»åŠ¡å·²å®Œæˆ ---\n")
    print(f"--- æ­¥éª¤ 4/4: æ’åºã€ç­›é€‰å¹¶ä¿å­˜æœ€ç»ˆçš„å‰æ²¿åˆ†ææŠ¥å‘Š ---")
    if not final_results:
        print("è­¦å‘Šï¼šæœªèƒ½æ‰¾åˆ°å¹¶æ€»ç»“ä»»ä½•ç›¸å…³çš„é¢†åŸŸå‰æ²¿è®ºæ–‡ã€‚"); return
    final_results.sort(key=lambda x: x['relevance_score'], reverse=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_results[:max_papers], f, ensure_ascii=False, indent=4)
    print(f"\nğŸ‰ Step 3 æˆåŠŸï¼å…±æ€»ç»“äº† {len(final_results)} ç¯‡å‰æ²¿è®ºæ–‡ã€‚")
    print(f"æŠ¥å‘Šä¸­å·²ä¿å­˜ç›¸å…³åº¦æœ€é«˜çš„ {len(final_results[:max_papers])} ç¯‡ã€‚")
    print(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file.resolve()}")

# ==============================================================================
# --- 5. è„šæœ¬å…¥å£ ---
# ==============================================================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Step 3: Find and summarize frontier research from arXiv.")
    parser.add_argument("--analysis_json_path", type=str, required=True)
    parser.add_argument("--output_path", type=str, required=True)
    parser.add_argument("--config", type=str, default='config.ini', help="Path to the configuration file.")
    args = parser.parse_args()
    config = configparser.ConfigParser()
    if not Path(args.config).exists():
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ '{args.config}' æœªæ‰¾åˆ°ï¼"); exit()
    config.read(args.config)
    try:
        max_papers = int(config['SETTINGS']['MAX_PAPERS_FRONTIER'])
        model_name = config['PATHS']['EMBEDDING_MODEL_PATH']
        relevance_threshold = float(config.get('SETTINGS', 'RELEVANCE_THRESHOLD', fallback=0.8))
        search_start_date = config.get('SETTINGS', 'ARXIV_SEARCH_START_DATE', fallback='2025-01')
    except (KeyError, ValueError) as e:
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ 'config.ini' æ ¼å¼ä¸æ­£ç¡®æˆ–ç¼ºå°‘å¿…è¦çš„é”®: {e}"); exit()
    if not os.getenv("DASHSCOPE_API_KEY"):
        print("é”™è¯¯ï¼šç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY æœªè®¾ç½®ã€‚"); exit()
    run_frontier_analysis(
        analysis_json_path=args.analysis_json_path, output_json_path=args.output_path,
        max_papers=max_papers, model_name=model_name,
        relevance_threshold=relevance_threshold, search_start_date=search_start_date
    )