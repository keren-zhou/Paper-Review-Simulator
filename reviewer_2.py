# -*- coding: utf-8 -*-

import openai
import os
import json
from pathlib import Path
import time
from typing import Dict, Any, List
from thefuzz import fuzz  # ç”¨äºæ¨¡ç³Šå­—ç¬¦ä¸²åŒ¹é…, ç¡®ä¿å·²è¿è¡Œ: pip install thefuzz
import argparse # å¯¼å…¥ argparse åº“ç”¨äºå¤„ç†å‘½ä»¤è¡Œå‚æ•°

# ==============================================================================
# --- 1. å…¨å±€é…ç½®ä¸å®¢æˆ·ç«¯åˆå§‹åŒ– ---
# ==============================================================================
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")
if not DASHSCOPE_API_KEY:
    # è¿™æ˜¯ä¸€ä¸ªå…œåº•æ£€æŸ¥ï¼Œä¸»æ§è„šæœ¬ä¼šç¡®ä¿API Keyå·²è®¾ç½®
    raise ValueError("é”™è¯¯ï¼šè¯·åœ¨æ‚¨çš„ç¯å¢ƒä¸­è®¾ç½® DASHSCOPE_API_KEY ç¯å¢ƒå˜é‡ï¼")

try:
    qwen_client = openai.OpenAI(
        api_key=DASHSCOPE_API_KEY,
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    )
    print("--- åƒé—® Qwen API å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ ---")
except Exception as e:
    print(f"åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆç”¨äºåƒé—®ï¼‰æ—¶å‡ºé”™: {e}")
    # åœ¨æµæ°´çº¿ä¸­ï¼Œå¦‚æœåˆå§‹åŒ–å¤±è´¥ï¼Œæœ€å¥½ç›´æ¥é€€å‡º
    exit(1)

# ==============================================================================
# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° (ä¸åŸç‰ˆä¿æŒä¸€è‡´) ---
# ==============================================================================

def load_required_data(summary_path: Path, frontier_path: Path) -> tuple[Dict[str, Any] | None, List[Dict[str, Any]] | None]:
    """åŠ è½½å®¡ç¨¿äººæ‰€éœ€çš„çŸ¥è¯†æ–‡ä»¶ã€‚"""
    print(f"\n--- æ­¥éª¤ 1/5: æ­£åœ¨åŠ è½½çŸ¥è¯†æ–‡ä»¶ ---")
    summary_data, frontier_data = None, None
    try:
        with open(summary_path, 'r', encoding='utf-8') as f:
            summary_data = json.load(f)
        print(f"  âœ… æˆåŠŸåŠ è½½è®ºæ–‡ç»“æ„åŒ–æ‘˜è¦: {summary_path.name}")
    except Exception as e:
        print(f"  âŒ é”™è¯¯: åŠ è½½æˆ–è§£æè®ºæ–‡æ‘˜è¦æ–‡ä»¶ '{summary_path}' å¤±è´¥: {e}")

    try:
        with open(frontier_path, 'r', encoding='utf-8') as f:
            frontier_data = json.load(f)
        print(f"  âœ… æˆåŠŸåŠ è½½é¢†åŸŸå‰æ²¿æŠ¥å‘Š: {frontier_path.name}")
    except Exception as e:
        print(f"  âŒ é”™è¯¯: åŠ è½½æˆ–è§£æå‰æ²¿æŠ¥å‘Š '{frontier_path}' å¤±è´¥: {e}")

    return summary_data, frontier_data

def filter_self_from_frontier(summary_data: Dict[str, Any], frontier_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ä»å‰æ²¿æŠ¥å‘Šä¸­è¿‡æ»¤æ‰ç”¨æˆ·è‡ªå·±çš„è®ºæ–‡ï¼Œé¿å…è‡ªæˆ‘æ¯”è¾ƒã€‚"""
    print("--- æ­¥éª¤ 2/5: æ­£åœ¨æ£€æŸ¥å¹¶ä»å‰æ²¿æŠ¥å‘Šä¸­æ’é™¤ç”¨æˆ·è®ºæ–‡è‡ªèº« ---")
    try:
        user_title = summary_data.get("paper_summary", {}).get("supporting_evidence", {}).get("title", "Unknown Title")
        if user_title == "Unknown Title":
             user_title = next((v for k, v in summary_data.items() if isinstance(v, str) and 'title' in k.lower()), "Unknown Title")
        
        filtered_list = []
        removed_count = 0
        for paper in frontier_data:
            frontier_title = paper.get('title', '')
            if fuzz.ratio(user_title.lower(), frontier_title.lower()) > 95:
                print(f"  â„¹ï¸  æ£€æµ‹åˆ°å¹¶å·²ç§»é™¤ç”¨æˆ·è®ºæ–‡è‡ªèº«: '{frontier_title}'")
                removed_count += 1
            else:
                filtered_list.append(paper)
        
        if removed_count == 0:
            print("  âœ… æœªåœ¨å‰æ²¿æŠ¥å‘Šä¸­å‘ç°ç”¨æˆ·è®ºæ–‡è‡ªèº«ã€‚")
            
        return filtered_list
    except Exception as e:
        print(f"  âš ï¸ è­¦å‘Š: åœ¨è¿‡æ»¤è‡ªèº«è®ºæ–‡æ—¶å‘ç”Ÿé”™è¯¯: {e}. å°†ä½¿ç”¨æœªç»è¿‡æ»¤çš„åˆ—è¡¨ã€‚")
        return frontier_data

def prepare_llm_input_context(summary_data: Dict[str, Any], filtered_frontier_data: List[Dict[str, Any]]) -> str:
    """å°†åŠ è½½çš„æ•°æ®æ•´åˆæˆä¸€ä¸ªç»“æ„æ¸…æ™°ã€é€‚åˆLLMåˆ†æçš„æ–‡æœ¬ä¸Šä¸‹æ–‡ã€‚"""
    print("--- æ­¥éª¤ 3/5: æ­£åœ¨ä¸ºAIå®¡ç¨¿é¡¾é—®å‡†å¤‡åˆ†æä¸Šä¸‹æ–‡ ---")
    
    paper_summary = summary_data.get("paper_summary", {})
    innovations = paper_summary.get("key_innovations", [])
    problem = paper_summary.get("problem_statement", "Not explicitly stated")

    innovations_text = "\n".join([f"- **{item.get('innovation_name', 'Unnamed Innovation')}**: {item.get('innovation_description', '')}" for item in innovations])

    user_paper_context = f"""
### Core Information of User's Paper
- **Problem Addressed**: {problem}
- **Claimed Key Innovations**:
{innovations_text}
"""

    frontier_papers_context = "\n### Background: State-of-the-Art Research in the Field\n"
    for i, paper in enumerate(filtered_frontier_data, 1):
        frontier_papers_context += f"""
---
**Frontier Paper {i}**:
- **Title**: {paper.get('title', 'N/A')}
- **Core Idea Summary**: {paper.get('frontier_summary', 'N/A')}
---
"""
    print("  âœ… ä¸Šä¸‹æ–‡å‡†å¤‡å®Œæˆã€‚")
    return user_paper_context + frontier_papers_context

def generate_novelty_review_md_with_qwen(context: str) -> str | None:
    """è°ƒç”¨åƒé—®å¤§æ¨¡å‹ï¼Œæ‰®æ¼”ä¸€ä¸ªå¯Œæœ‰æ´å¯ŸåŠ›çš„å®¡ç¨¿é¡¾é—®ï¼Œç”Ÿæˆä¸€ä»½å»ºè®¾æ€§çš„è‹±æ–‡MarkdownæŠ¥å‘Šã€‚"""
    # ... (æ­¤å‡½æ•°å†…éƒ¨çš„ Prompt å’Œ API è°ƒç”¨é€»è¾‘ä¿æŒä¸å˜) ...
    prompt = f"""
You are a top-tier AI research advisor with extensive experience and a sharp eye for detail. Your task is not merely to criticize a paper, but to act as a senior mentor. Based on the provided "Core Information of User's Paper" and the "State-of-the-Art Research Background," you must provide the author with a profound, forward-looking novelty assessment report. Your goal is to help them anticipate and prepare for the tough questions they might face during the rebuttal phase.

Your output MUST be the complete content for a **Markdown (.md)** file, written in professional, academic English.

Please strictly follow this Markdown structure for your analysis and report:

# Novelty and Contribution Assessment Report (Reviewer #2 Perspective)

## 1. Overall Originality Assessment
*   Provide a concise, insightful overall evaluation here. Where does the originality of this paper lie? Is it a completely new idea, a clever combination of existing techniques, or an incremental improvement upon prior work? Clearly position it within the current academic landscape.

## 2. Detailed Innovation Analysis
*   Analyze each of the user's "Claimed Key Innovations" one by one.
*   Compare each innovation against the "State-of-the-Art Research Background." Is there conceptual overlap? Or does it address a specific aspect overlooked by recent work? Your analysis must be specific and well-supported by the provided context.

## 3. Problem Timeliness & Research Motivation
*   Evaluate whether the problem this paper addresses is still an open and significant challenge in the field.
*   Based on the frontier research, are there new technological trends or paradigms that have shifted how this problem is typically approached? Is the paper's motivation sufficiently strong and well-argued?

## 4. Potential Discussion Points for Rebuttal
*   **Crucial Note**: Your goal is NOT to directly criticize the author for a "lack of baseline comparisons," as code for many state-of-the-art works is often unavailable. Instead, you must identify the **1-3 most relevant papers** from the frontier research that a human reviewer is most likely to bring up.
*   For each identified paper, simulate a reviewer's tone and pose a specific, pointed question. For example: "A reviewer might ask: How does your method fundamentally differ from, and what are its advantages over, [Frontier Paper X] in terms of [a specific aspect]?"
*   Then, provide the author with a direction for their thinking or a suggestion to help them formulate a response that emphasizes the uniqueness of their work. This section is about preparation and strategy, not criticism.

---
[Analysis Materials]
{context}
---

Now, please begin writing your Markdown assessment report. Your output must be the complete Markdown text, starting with `# Novelty and Contribution Assessment Report`.
"""
    print("--- æ­¥éª¤ 4/5: æ­£åœ¨è°ƒç”¨åƒé—®APIç”Ÿæˆè‹±æ–‡æ–°é¢–æ€§è¯„ä¼°MDæŠ¥å‘Š... ---")
    start_time = time.time()
    try:
        response = qwen_client.chat.completions.create(
            model="qwen-plus",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=4096,
            temperature=0.2,
        )
        end_time = time.time()
        print(f"  âœ… æˆåŠŸæ”¶åˆ°APIå“åº”ï¼Œè€—æ—¶ {end_time - start_time:.2f} ç§’ã€‚")
        return response.choices[0].message.content
    except Exception as e:
        print(f"  âŒ é”™è¯¯ï¼šè°ƒç”¨åƒé—®APIæ—¶å¤±è´¥: {e}")
        return None

def save_report_md(report_content: str, output_path: Path):
    """å°†ç”Ÿæˆçš„MarkdownæŠ¥å‘Šå†…å®¹ä¿å­˜ä¸º.mdæ–‡ä»¶ã€‚"""
    print(f"--- æ­¥éª¤ 5/5: æ­£åœ¨ä¿å­˜Markdownè¯„ä¼°æŠ¥å‘Š ---")
    try:
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        print(f"  ğŸ‰ è¯„ä¼°æŠ¥å‘Šå·²æˆåŠŸä¿å­˜åˆ°: {output_path.resolve()}")
    except Exception as e:
        print(f"  âŒ é”™è¯¯: ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

# ==============================================================================
# --- 3. ä¸»æµç¨‹æ§åˆ¶å™¨ (å·²ä¿®æ”¹ä»¥é€‚åº”æµæ°´çº¿) ---
# ==============================================================================

def run_novelty_assessment(summary_json_path: str, frontier_report_path: str, output_md_path: str):
    """
    åè°ƒæ•´ä¸ªæ–°é¢–æ€§è¯„ä¼°æµç¨‹çš„ä¸»å‡½æ•°ã€‚
    æ­¤å‡½æ•°ç°åœ¨æ¥æ”¶ç²¾ç¡®çš„è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œè€Œä¸æ˜¯ç›®å½•ã€‚
    """
    summary_file = Path(summary_json_path)
    frontier_file = Path(frontier_report_path)
    output_file = Path(output_md_path)
    
    # ç¡®ä¿è¾“å‡ºæ–‡ä»¶çš„çˆ¶ç›®å½•å­˜åœ¨
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    summary_data, frontier_data = load_required_data(summary_file, frontier_file)
    if not summary_data or not frontier_data:
        print("\nâŒ æµç¨‹ç»ˆæ­¢ï¼šç¼ºå°‘ä¸€ä¸ªæˆ–å¤šä¸ªå¿…è¦çš„çŸ¥è¯†æ–‡ä»¶ã€‚")
        return

    filtered_frontier = filter_self_from_frontier(summary_data, frontier_data)
    
    llm_context = prepare_llm_input_context(summary_data, filtered_frontier)
    
    md_report = generate_novelty_review_md_with_qwen(llm_context)
    
    if md_report:
        # ç›´æ¥å°†æŠ¥å‘Šä¿å­˜åˆ°ç”±ä¸»æ§è„šæœ¬æŒ‡å®šçš„è·¯å¾„
        save_report_md(md_report, output_file)
    else:
        print("\nâŒ æµç¨‹ç»ˆæ­¢ï¼šæœªèƒ½ç”Ÿæˆæ–°é¢–æ€§è¯„ä¼°æŠ¥å‘Šã€‚")

# ==============================================================================
# --- 4. è„šæœ¬å…¥å£ (å·²ä¿®æ”¹ä¸ºæ¥æ”¶å‘½ä»¤è¡Œå‚æ•°) ---
# ==============================================================================
if __name__ == "__main__":
    # è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description="Reviewer 2: åŸºäºå‰æ²¿ç ”ç©¶è¿›è¡Œè®ºæ–‡æ–°é¢–æ€§è¯„ä¼°ã€‚")
    parser.add_argument("--summary_json_path", type=str, required=True, help="æ¥è‡ª Step 2 çš„ç»¼åˆåˆ†æ JSON æ–‡ä»¶è·¯å¾„ã€‚")
    parser.add_argument("--frontier_report_path", type=str, required=True, help="æ¥è‡ª Step 4 çš„å‰æ²¿åˆ†ææŠ¥å‘Š JSON æ–‡ä»¶è·¯å¾„ã€‚")
    parser.add_argument("--output_path", type=str, required=True, help="ç”¨äºä¿å­˜æ–°é¢–æ€§å®¡ç¨¿æŠ¥å‘Šçš„ Markdown æ–‡ä»¶è·¯å¾„ã€‚")
    parser.add_argument("--config", type=str, help="Path to the configuration file (accepted but not used).")
    # è§£æä»å‘½ä»¤è¡Œä¼ å…¥çš„å‚æ•°
    args = parser.parse_args()

    # æ£€æŸ¥æ‰€æœ‰å¿…è¦çš„è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œæä¾›æ›´æ¸…æ™°çš„é”™è¯¯æç¤º
    input_paths = [args.summary_json_path, args.frontier_report_path]
    if not all(Path(p).exists() for p in input_paths):
         print("="*60)
         print("é”™è¯¯ï¼šä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ã€‚è¯·æ£€æŸ¥ main.py ä¼ é€’çš„è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
         print(f"  - è®ºæ–‡æ‘˜è¦è·¯å¾„: '{args.summary_json_path}' (å­˜åœ¨: {Path(args.summary_json_path).exists()})")
         print(f"  - å‰æ²¿æŠ¥å‘Šè·¯å¾„: '{args.frontier_report_path}' (å­˜åœ¨: {Path(args.frontier_report_path).exists()})")
         print("="*60)
    else:
        # è°ƒç”¨ä¸»æµç¨‹å‡½æ•°ï¼Œä¼ å…¥è§£æåçš„å‚æ•°
        run_novelty_assessment(
            summary_json_path=args.summary_json_path,
            frontier_report_path=args.frontier_report_path,
            output_md_path=args.output_path
        )